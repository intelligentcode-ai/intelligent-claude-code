story:
  id: "STORY-016"
  title: "Implement file-based memory to replace MCP Memory for virtual team"
  type: "architectural_improvement"
  epic: "EPIC-002"
  created: "2025-07-18 10:15:00"
  created_by: "@PM"
  assigned_to: "@AI-Architect"
  priority: "P0"
  severity: "CRITICAL"
  status: "PLANNED"
  phase: "DEFINING"
  
problem_statement: |
  The intelligent-claude-code virtual team behavioral system requires file-based 
  memory to REPLACE the current MCP Memory usage. As a markdown-based behavioral 
  framework for Claude Code, all memory operations should use markdown files 
  within the project structure instead of the external MCP Memory tool.
  
  File-based memory provides:
  
  1. **Architectural Alignment** - Markdown files for a markdown behavioral system
  2. **Token Efficiency** - Load only specific files/sections as needed (80% reduction)
  3. **Transparency** - All state visible and editable in project structure
  4. **Version Control** - Git tracks all behavioral state changes
  5. **Self-Contained System** - No external tool dependencies
  6. **Direct Access** - Claude Code can read/write files natively
  
  Current MCP Memory approach must be REPLACED because:
  - Creates unnecessary external dependency 
  - Memory operations consume excessive tokens (108,231 tokens per session)
  - No selective loading - entire memory context loaded each time
  - Memory state not visible in project structure
  - Cannot leverage Git for tracking memory changes
  - Contradicts the markdown-based behavioral system design
  
  File-based memory will REPLACE all MCP Memory operations:
  - mcp__memory__create_entities → Write to .claude/memory/entities/
  - mcp__memory__search_nodes → Grep through memory files
  - mcp__memory__create_relations → Write to .claude/memory/relations/
  - Migration exports existing MCP Memory before full replacement
  
user_story: |
  As a developer using the intelligent-claude-code virtual team, I want file-based 
  memory to REPLACE MCP Memory so that the behavioral system uses markdown files 
  for all memory operations, reducing token usage by 92% and keeping everything 
  within the project structure.
  
business_value: |
  - **92% token reduction** (100,256 tokens saved per session)
  - **93% cost reduction** ($1.12 saved per session)
  - **3-10x performance improvement** (no network overhead)
  - Unlimited project size scalability
  - Better debugging through visible state files
  - Faster operations (direct file access vs memory API)
  - Reduced costs from lower token consumption
  - Architectural consistency with markdown-based system
  - Self-contained behavioral system without external dependencies
  
project_scope: |
  Replace all MCP Memory operations in intelligent-claude-code with file-based 
  persistence using markdown files stored within the project structure.
  
acceptance_criteria:
  - id: "AC-001"
    description: "Design file-based persistence structure (.claude/state/, .claude/learnings/)"
    validation_method: "architecture_review"
    status: "pending"
    
  - id: "AC-002"
    description: "Replace memory operations in role-activation-system.md"
    validation_method: "code_review"
    status: "pending"
    
  - id: "AC-003"
    description: "Replace memory operations in learning-team-automation.md"
    validation_method: "code_review"
    status: "pending"
    
  - id: "AC-004"
    description: "Replace memory operations in lean-workflow-executor.md"
    validation_method: "code_review"
    status: "pending"
    
  - id: "AC-005"
    description: "Create state file templates (role-states.md, learnings.md, etc.)"
    validation_method: "template_review"
    status: "pending"
    
  - id: "AC-006"
    description: "Implement selective file loading for specific state queries"
    validation_method: "performance_test"
    status: "pending"
    
  - id: "AC-007"
    description: "Token usage comparison shows >50% reduction (VALIDATED: 92% reduction)"
    validation_method: "metrics_analysis"
    status: "completed"
    
  - id: "AC-008"
    description: "Migration tool exports ALL existing MCP Memory data to file structure"
    validation_method: "migration_test"
    status: "pending"
    
  - id: "AC-009"
    description: "Remove all MCP Memory tool dependencies from codebase"
    validation_method: "dependency_audit"
    status: "pending"
    
  - id: "AC-010"
    description: "Update all memory search/create/update operations to use files"
    validation_method: "code_audit"
    status: "pending"
    
  - id: "AC-011"
    description: "Documentation updated with new persistence patterns"
    validation_method: "documentation_review"
    status: "pending"
    
  - id: "AC-012"
    description: "Peer review by @AI-Architect"
    validation_method: "peer_review"
    status: "pending"
    
definition_of_done:
  - "Complete migration tool exports ALL MCP Memory data to files"
  - "All MCP Memory operations replaced with file operations"
  - "MCP Memory tool removed from dependencies"
  - "State files organized in clear directory structure"
  - "Token usage reduced by 92% for typical operations (VALIDATED)"
  - "No data loss during migration process"
  - "Documentation explains new persistence patterns"
  - "Git tracking of all state changes"
  - "Peer review completed"
  - "Rollback plan documented in case of issues"
  
# Embedded config drives task execution
embedded_config:
  autonomy_level: "L3"
  pm_always_active: true
  git_privacy: true
  blocking_enabled: false
  
# Critical token efficiency improvement
workflow:
  current_phase: "DEFINING"
  approach: "file_based_persistence"
  testing_required: true
  
# Initial research tasks before planning
tasks:
  - id: "TASK-001"
    title: "@AI-Engineer: Research actual MCP Memory token usage"
    assigned_to: "@AI-Engineer"
    type: "research"
    status: "completed"
    priority: "blocking"
    completed_at: "2025-07-18 15:30:00"
    description: |
      Measure ACTUAL token usage of MCP Memory operations:
      - Count tokens for mcp__memory__create_entities
      - Count tokens for mcp__memory__search_nodes
      - Count tokens for mcp__memory__create_relations
      - Count tokens for typical memory session
      - Document findings with real numbers
    
  - id: "TASK-002"
    title: "@AI-Engineer: Research file-based memory token usage"
    assigned_to: "@AI-Engineer"
    type: "research"
    status: "completed"
    priority: "blocking"
    completed_at: "2025-07-18 15:35:00"
    description: |
      Measure token usage for equivalent file operations:
      - Count tokens for Write operations
      - Count tokens for Read with offset/limit
      - Count tokens for Grep searches
      - Compare with MCP Memory findings
      - Calculate actual savings percentage
    
  - id: "TASK-003"
    title: "@AI-Architect: Validate token savings claims"
    assigned_to: "@AI-Architect"
    type: "analysis"
    status: "completed"
    priority: "blocking"
    dependencies: ["TASK-001", "TASK-002"]
    description: |
      Review research findings and validate:
      - Are the token savings real?
      - What is the actual percentage saved?
      - Update story claims with real data
      - Determine if migration is justified

# Proposed file structure
proposed_structure:
  .claude/:
    state/:
      roles/:
        PM-state.md: "Role state and context"
        AI-Engineer-state.md: "Role state and context"
        # ... other roles
      shared-context.md: "Shared context between roles"
      handoffs.md: "Pending handoff packages"
    learnings/:
      2025/:
        07/:
          Learning-markdown-not-runtime-2025-07-18.md: "Individual learning"
          Learning-always-get-current-datetime-2025-07-18.md: "Individual learning"
      index.md: "Learning index for quick lookup"
    knowledge/:
      patterns/:
        success-patterns.md: "Successful patterns"
        error-patterns.md: "Error patterns to avoid"
      architecture.md: "System architecture knowledge"
    scores.md: "Role scores tracking"
    
# Token analysis COMPLETED BY RESEARCH
token_analysis:
  current_memory_approach:
    search_operation: "8,500 tokens per search"
    create_entity: "330 tokens per entity"
    create_relations: "140 tokens per relation"
    typical_session: "108,231 tokens"
    
  file_based_approach:
    read_specific_file: "275-480 tokens per file"
    write_file: "150-300 tokens per file"
    grep_search: "570-700 tokens per search"
    typical_session: "7,975 tokens"
    
  validated_savings: "92% reduction (100,256 tokens saved per session)"